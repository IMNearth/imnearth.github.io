---
title: "Research"
permalink: /research/
author_profile: true
---


[Preprints](#preps) &nbsp; &nbsp; - &nbsp; &nbsp; [Publications](#pubs) &nbsp; &nbsp; - &nbsp; &nbsp; [Google Scholar Profile (Up to date)](https://scholar.google.com/citations?user=wQCKDNQAAAAJ&hl=en)

*\* indicates equal contributions.*  

<h2 id="preps">
Preprints
</h2>

[P3] **Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning** [arXiv:2509.22746](https://arxiv.org/pdf/2509.22746?) \
&emsp; &emsp; ðŸ”¥Under Review. \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Zejun Li, Yingxiu Zhao, **Jiwen Zhang**, Siyuan Wang, Yang Yao, Runzhou Zhao, Jun Song, Bo Zheng, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Chain of thought, visual reasoning, multi-modal large language models.

[P2] **"AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs"** [arXiv:2505.21389](https://arxiv.org/pdf/2505.21389?) \
&emsp; &emsp; ðŸ”¥Under Review. \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Xuanwen Ding, Chengjun Pan, Zejun Li, **Jiwen Zhang**, Siyuan Wang, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Efficient benchmarking, multi-modal large language models, judging agent.

[P1] **"Continuous or discrete, that is the question: A survey on large multi-modal models from the perspective of input-output space extension"** [PrePrints:202411.0685](https://www.preprints.org/frontend/manuscript/8267896f227ef3f94a645a6b196f0b46/download_pub) \
&emsp; &emsp; ðŸ”¥Under Review. \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Zejun Li, **Jiwen Zhang**, Dianyi Wang, Ye Wang, Xuanjing Huang, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Survey, large multi-modal models, input-output space extension.


<h2 id="pubs">
Publications
</h2>

<h3 id="conf">
Conference Proceedings
</h3>


[C6] **"UI-Hawk: Unleashing the Screen Stream Understanding for Mobile GUI Agents"** [CameraReady](https://aclanthology.org/2025.naacl-long.192.pdf) \
&emsp; &emsp; ðŸ“’<u>Venue</u>: Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (**EMNLP**) (2025, November) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: **Jiwen Zhang**\*, Ya-Qi Yu\*, Minghui Liao, Wentao Li, Jihao Wu, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: GUI navigation, large multi-modal models, supervised fine-tuning.

[C5] **"Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models"** [CameraReady](https://aclanthology.org/2025.naacl-long.192.pdf)
&emsp; &emsp; ðŸ“’<u>Venue</u>: Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (**NAACL**) (2025, April) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Zejun Li\*, Ruipu Luo\*, **Jiwen Zhang**, Minghui Qiu, Xuan-Jing Huang, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Large multi-modal models, visual reasoning, chain-of-thought.

[C4] **"Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks"** [CameraReady](https://dl.acm.org/doi/10.1145/3664647.3681529) \
&emsp; &emsp; ðŸ“’<u>Venue</u>: Proceedings of the 32nd ACM International Conference on Multimedia (**ACM MM**) (2024, October) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Zejun Li\*, Ye Wang\*, Mengfei Du\*, Qingwen Liu\*, Binhao Wu\*, **Jiwen Zhang**,\* Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, Zhongyu Wei, Xuanjing Huang. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Large multi-modal models, evaluation.

[C3] **"Android in the zoo: Chain-of-action-thought for gui agents"** [CameraReady](https://aclanthology.org/2024.findings-emnlp.702.pdf)
&emsp; &emsp; ðŸ“’<u>Venue</u>: Findings of the Association for Computational Linguistics: EMNLP 2024 (**EMNLP Findings**) (2024, November) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: **Jiwen Zhang**, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: GUI navigation, prompt engineering, supervised fine-tuning.

[C2] **"DELAN: Dual-level alignment for vision-and-language navigation by cross-modal contrastive learning"** [CameraReady](https://aclanthology.org/2024.lrec-main.411.pdf) \
&emsp; &emsp; ðŸ“’<u>Venue</u>: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (**LREC-COLING**) (2024, May) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Mengfei Du\*, Binhao Wu\*, **Jiwen Zhang**, Zhihao Fan, Zejun Li, Ruipu Luo, Xuan-Jing Huang, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Contrastive learning, vision-language navigation, cross-modal alignment.

[C1] **"Curriculum Learning for Vision-and-Language Navigation"** [CameraReady](https://proceedings.neurips.cc/paper_files/paper/2021/file/6f0442558302a6ededff195daf67f79b-Paper.pdf) \
&emsp; &emsp; ðŸ“’<u>Venue</u>: Advances in Neural Information Processing Systems 34 (**NeurIPS**) (2021, December) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: **Jiwen Zhang**, Zhongyu Wei, Jianqing Fan, Jiajie Peng. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Curriculum learning, vision-language navigation, embedied agent.



<h3 id="repo">
Technical Reports
</h3>


[T2] **"Texthawk2: A large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens"** [arXiv:2410.05261](https://arxiv.org/pdf/2410.05261) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Ya-Qi Yu, Minghui Liao, **Jiwen Zhang**, Jihao Wu. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Large multi-modal models, pre-training, text recognition and grounding.

[T1] **"Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making"** [arXiv:2307.08016](https://arxiv.org/pdf/2307.08016) \
&emsp; &emsp; ðŸ‘¤<u>Authors</u>: Ruipu Luo, **Jiwen Zhang**, Zhongyu Wei. \
&emsp; &emsp; ðŸ”‘<u>Keywords</u>: Vision language dicison making, hybrid learning.

